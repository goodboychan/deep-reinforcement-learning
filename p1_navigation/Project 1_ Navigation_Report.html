<!DOCTYPE html><html><head><meta charset="utf-8"><title>Project 1: Navigation.md</title><style></style></head><body id="preview">
<h1 class="code-line" data-line-start=4 data-line-end=5><a id="Project_1_Navigation_4"></a>Project 1: Navigation</h1>
<h3 class="code-line" data-line-start=6 data-line-end=7><a id="Introduction_6"></a>Introduction</h3>
<p class="has-line-data" data-line-start="8" data-line-end="10">In this project, I tried to implement Deep Q-Network (DQN) Agent, one of most common value-based deep reinforcement learning algorithm.<br>
The problem I solve is to collect yellow bananas in the large,square world.</p>
<p class="has-line-data" data-line-start="11" data-line-end="12"><img src="https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif" alt="Trained Agent" title="Trained Agent"></p>
<h4 class="code-line" data-line-start=13 data-line-end=14><a id="The_environment_13"></a>The environment</h4>
<p class="has-line-data" data-line-start="15" data-line-end="16">A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. Thus, the <strong>goal of your agent</strong> is to collect as many yellow bananas as possible while avoiding blue bananas.</p>
<p class="has-line-data" data-line-start="17" data-line-end="18">The state space has <strong>37</strong> dimensions and contains the agent’s velocity, along with ray-based perception of objects around agent’s forward direction. Given this information, the agent has to learn how to best select actions. <strong>Four</strong> discrete actions are available, corresponding to:</p>
<ul>
<li class="has-line-data" data-line-start="18" data-line-end="19"><strong><code>0</code></strong> - move forward.</li>
<li class="has-line-data" data-line-start="19" data-line-end="20"><strong><code>1</code></strong> - move backward.</li>
<li class="has-line-data" data-line-start="20" data-line-end="21"><strong><code>2</code></strong> - turn left.</li>
<li class="has-line-data" data-line-start="21" data-line-end="23"><strong><code>3</code></strong> - turn right.</li>
</ul>
<p class="has-line-data" data-line-start="23" data-line-end="25">The task is episodic, and in order to solve the environment, your agent must get an average score of +13 over 100 consecutive episodes.<br>
And my agent can get 24 rewards with trained model.</p>
<h4 class="code-line" data-line-start=26 data-line-end=27><a id="Perparation_26"></a>Perparation</h4>
<p class="has-line-data" data-line-start="28" data-line-end="30">This environment is built on <a href="https://github.com/Unity-Technologies/ml-agents">Unity ml-agents</a>. Unlike OpenAI gym environment(of course, it has same OpenAI gym-styled environemt), it runs on independent executable frames, and requires several libraries related on graphics.<br>
To build the model on ml-agents, some python packages are required to install</p>
<ul>
<li class="has-line-data" data-line-start="31" data-line-end="32">tensorflow 1.7.1</li>
<li class="has-line-data" data-line-start="32" data-line-end="33">torch 0.4.0</li>
<li class="has-line-data" data-line-start="33" data-line-end="34">Pillow</li>
<li class="has-line-data" data-line-start="34" data-line-end="35">jupyter</li>
<li class="has-line-data" data-line-start="35" data-line-end="37">and so on</li>
</ul>
<p class="has-line-data" data-line-start="37" data-line-end="38">There are some <a href="https://github.com/udacity/deep-reinforcement-learning/blob/master/python/requirements.txt">requirements</a> to keep in mind. All you need to is make virtual environment via anaconda, and install that required packages through <code>pip</code>.</p>
<p class="has-line-data" data-line-start="39" data-line-end="41">Once you prepare the development environment, please follow the step in <a href="https://github.com/udacity/deep-reinforcement-learning/tree/master/p1_navigation#getting-started">here</a>!<br>
After you have followed the instructions above, open <code>Navigation.ipynb</code> (located in the <code>p1_navigation/</code>) through jupyter notebook and follow the instructions to learn how to use the Python API to control the agent.</p>
<p class="has-line-data" data-line-start="42" data-line-end="43"><a href="https://www.youtube.com/watch?v=ltz2GhFv04A&amp;t=1s"><img src="https://img.youtube.com/vi/ltz2GhFv04A/0.jpg" alt="Instruction"></a></p>
<h3 class="code-line" data-line-start=44 data-line-end=45><a id="Process_44"></a>Process</h3>
<p class="has-line-data" data-line-start="46" data-line-end="47">I followed several steps to implement DQN agent. All codes and archiecture is based on previous code example ‘DQN on LunarLander-v2’</p>
<ol>
<li class="has-line-data" data-line-start="48" data-line-end="49">Define neural network architecture</li>
</ol>
<ul>
<li class="has-line-data" data-line-start="49" data-line-end="55">I tried to use simple neural network that consists of fully-connected layer. Since this environment has discrete state space(37 dimensions) and discrete action space(4), so we don’t need complex architecture. So I used 4 fc layers (state-in-value-out architecture) (64-128-64 nodes for each hidden layer) and ReLU activation function.
<ul>
<li class="has-line-data" data-line-start="50" data-line-end="51">Input Layer: Fully Connected Layer (in:state_size, out:64) with ReLU activation</li>
<li class="has-line-data" data-line-start="51" data-line-end="52">1st hidden Layer: Fully Connected Layer (in:64, out:128) with ReLU activation</li>
<li class="has-line-data" data-line-start="52" data-line-end="53">2nd hidden Layer: Fully Connected Layer (in:128, out:64) with ReLU activation</li>
<li class="has-line-data" data-line-start="53" data-line-end="55">Output Layer: Fully Connected Layer (in:64, out:action_size)</li>
</ul>
</li>
</ul>
<ol start="2">
<li class="has-line-data" data-line-start="55" data-line-end="56">Define Experience Replay buffer.</li>
</ol>
<ul>
<li class="has-line-data" data-line-start="56" data-line-end="57">DQN use off-policy TD target. That is, we can train it with sampling data in offline manners. And to enhance the sampling efficiency, we can store the experience tuple that face previousely and sample it for training.</li>
<li class="has-line-data" data-line-start="57" data-line-end="59">Usually, sampling size (also known as batch size) is a hyperparameter.</li>
</ul>
<ol start="3">
<li class="has-line-data" data-line-start="59" data-line-end="60">Define Agent</li>
</ol>
<ul>
<li class="has-line-data" data-line-start="60" data-line-end="61">In the original DQN paper, there are two identical networks, local network and target network. While the agent is trained with local network, the target network is fixed to make fixed target. It makes RL problem a sort of Supervised Learning. Of course, it requires several hyperparameters such as how long the period do we have to update the target network, or total size of epoch, buffer size to store and sample the experience tuple and so on.</li>
<li class="has-line-data" data-line-start="61" data-line-end="62">At first, I used same hyperparameter on code example ‘LunarLander-v2’. But I found that the state space in ‘BananaWorld’ is larger than ‘LunarLander’. So I increased the <strong>batch size</strong> from 64 to 128. And I thought that as the larger as the state space is, the training is much harder. So it requires to fix the target much longer, and I changed the <strong>update period</strong> from 4 to 8</li>
<li class="has-line-data" data-line-start="62" data-line-end="63">Also, I used another hyperparameter configuration like this:</li>
</ul>
<pre><code class="has-line-data" data-line-start="64" data-line-end="69" class="language-python">BUFFER_SIZE = int(<span class="hljs-number">1e5</span>)  <span class="hljs-comment"># replay buffer size</span>
GAMMA = <span class="hljs-number">0.99</span>            <span class="hljs-comment"># discount factor</span>
TAU = <span class="hljs-number">1e-3</span>              <span class="hljs-comment"># for soft update of target parameters</span>
LR = <span class="hljs-number">5e-4</span>               <span class="hljs-comment"># learning rate </span>
</code></pre>
<ul>
<li class="has-line-data" data-line-start="69" data-line-end="71">I follow the same agent architecture from previous code example.</li>
</ul>
<ol start="4">
<li class="has-line-data" data-line-start="71" data-line-end="72">Training</li>
</ol>
<ul>
<li class="has-line-data" data-line-start="72" data-line-end="73">At last, the agent is played in training mode. I ran the 2000 episodes and when the average reward in 100 consequence step is larger than 16.3, then the training step stopped.</li>
<li class="has-line-data" data-line-start="73" data-line-end="74">When the training is terminated successfully, I save the network model. We don’t have whole network model, but model’s weight with dictionary type.</li>
<li class="has-line-data" data-line-start="74" data-line-end="75">It takes almost 1000 episodes.</li>
<li class="has-line-data" data-line-start="75" data-line-end="77">plot for rewards per episode</li>
</ul>
<p class="has-line-data" data-line-start="77" data-line-end="78"><img src="https://raw.githubusercontent.com/goodboychan/deep-reinforcement-learning/master/p1_navigation/DQN_result.png" alt="image2" title="Fig. reward per episode"></p>
<ol start="5">
<li class="has-line-data" data-line-start="79" data-line-end="80">Measure the performance under test mode.</li>
</ol>
<ul>
<li class="has-line-data" data-line-start="80" data-line-end="81">I load the network model’s weight on initial local network, and test it until its done.</li>
<li class="has-line-data" data-line-start="81" data-line-end="83">The trained model can get 24.0 reward.</li>
</ul>
<h3 class="code-line" data-line-start=83 data-line-end=84><a id="Lesson_Learned_83"></a>Lesson Learned</h3>
<p class="has-line-data" data-line-start="85" data-line-end="86">Of course, neural network architecture is important to solve the problem, but one of challengeable problem is tuning the hyperparameter. And it depends on the the environment that agent faced. It will be helpful to use traditional hyperparameter optimization like grid search or bayes optimization.</p>
</body></html>